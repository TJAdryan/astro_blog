[
  {
    "id": "http://arxiv.org/abs/1501.04832v1",
    "title": "Big Data: How Geo-information Helped Shape the Future of Data Engineering",
    "summary": "Very large data sets are the common rule in automated mapping, GIS, remote sensing, and what we can name geo-information. Indeed, in 1983 Landsat was already delivering gigabytes of data, and other sensors were in orbit or ready for launch, and a tantamount of cartographic data was being digitized. The retrospective paper revisits several issues that geo-information sciences had to face from the early stages on, including: structure ( to bring some structure to the data registered from a sampled signal, metadata); processing (huge amounts of data for big computers and fast algorithms); uncertainty (the kinds of errors, their quantification); consistency (when merging different sources of data is logically allowed, and meaningful); ontologies (clear and agreed shared definitions, if any kind of decision should be based upon them). All these issues are the background of Internet queries, and the underlying technology has been shaped during those years when geo-information engineering emerged.",
    "link": "http://arxiv.org/abs/1501.04832v1",
    "source": "arXiv",
    "topic": "Data Engineering",
    "dateAdded": "2026-01-04T22:23:53.718Z"
  },
  {
    "id": "37849926",
    "title": "Application of the endogenous CRISPR-Cas type I-D system for genetic engineering in the thermoacidophilic archaeon Sulfolobus acidocaldarius.",
    "summary": "No abstract available via summary API. Please click link to read.",
    "link": "https://pubmed.ncbi.nlm.nih.gov/37849926/",
    "source": "PubMed",
    "topic": "CRISPR",
    "dateAdded": "2026-01-19T13:17:04.230Z"
  },
  {
    "id": "http://arxiv.org/abs/2511.05821v1",
    "title": "PyGress: Tool for Analyzing the Progression of Code Proficiency in Python OSS Projects",
    "summary": "Assessing developer proficiency in open-source software (OSS) projects is essential for understanding project dynamics, especially for expertise. This paper presents PyGress, a web-based tool designed to automatically evaluate and visualize Python code proficiency using pycefr, a Python code proficiency analyzer. By submitting a GitHub repository link, the system extracts commit histories, analyzes source code proficiency across CEFR-aligned levels (A1 to C2), and generates visual summaries of individual and project-wide proficiency. The PyGress tool visualizes per-contributor proficiency distribution and tracks project code proficiency progression over time. PyGress offers an interactive way to explore contributor coding levels in Python OSS repositories. The video demonstration of the PyGress tool can be found at https://youtu.be/hxoeK-ggcWk, and the source code of the tool is publicly available at https://github.com/MUICT-SERU/PyGress.",
    "link": "http://arxiv.org/abs/2511.05821v1",
    "source": "arXiv",
    "topic": "Python",
    "dateAdded": "2026-01-19T13:17:04.456Z"
  },
  {
    "id": "38066376",
    "title": "Genetic Engineering of Therapeutic Phages Using Type III CRISPR-Cas Systems.",
    "summary": "No abstract available via summary API. Please click link to read.",
    "link": "https://pubmed.ncbi.nlm.nih.gov/38066376/",
    "source": "PubMed",
    "topic": "CRISPR",
    "dateAdded": "2026-01-26T13:16:57.194Z"
  },
  {
    "id": "http://arxiv.org/abs/2601.13139v1",
    "title": "From Human to Machine Refactoring: Assessing GPT-4's Impact on Python Class Quality and Readability",
    "summary": "Refactoring is a software engineering practice that aims to improve code quality without altering program behavior. Although automated refactoring tools have been extensively studied, their practical applicability remains limited. Recent advances in Large Language Models (LLMs) have introduced new opportunities for automated code refactoring. The evaluation of such an LLM-driven approach, however, leaves unanswered questions about its effects on code quality. In this paper, we present a comprehensive empirical study on LLM-driven refactoring using GPT-4o, applied to 100 Python classes from the ClassEval benchmark. Unlike prior work, our study explores a wide range of class-level refactorings inspired by Fowler's catalog and evaluates their effects from three complementary perspectives: (i) behavioral correctness, verified through unit tests; (ii) code quality, assessed via Pylint, Flake8, and SonarCloud; and (iii) readability, measured using a state-of-the-art readability tool. Our findings show that GPT-4o generally produces behavior-preserving refactorings that reduce code smells and improve quality metrics, albeit at the cost of decreased readability. Our results provide new evidence on the capabilities and limitations of LLMs in automated software refactoring, highlighting directions for integrating LLMs into practical refactoring workflows.",
    "link": "http://arxiv.org/abs/2601.13139v1",
    "source": "arXiv",
    "topic": "Python",
    "dateAdded": "2026-01-26T13:16:57.469Z"
  },
  {
    "id": "http://arxiv.org/abs/2504.10950v2",
    "title": "Unveiling Challenges for LLMs in Enterprise Data Engineering",
    "summary": "Large Language Models (LLMs) promise to automate data engineering on tabular data, offering enterprises a valuable opportunity to cut the high costs of manual data handling. But the enterprise domain comes with unique challenges that existing LLM-based approaches for data engineering often overlook, such as large table sizes, more complex tasks, and the need for internal knowledge. To bridge these gaps, we identify key enterprise-specific challenges related to data, tasks, and background knowledge and extensively evaluate how they affect data engineering with LLMs. Our analysis reveals that LLMs face substantial limitations in real-world enterprise scenarios, with accuracy declining sharply. Our findings contribute to a systematic understanding of LLMs for enterprise data engineering to support their adoption in industry.",
    "link": "http://arxiv.org/abs/2504.10950v2",
    "source": "arXiv",
    "topic": "Data Engineering",
    "dateAdded": "2026-02-02T13:24:58.660Z"
  },
  {
    "id": "36842543",
    "title": "CRISPR/Cas advancements for genome editing, diagnosis, therapeutics, and vaccine development for Plasmodium parasites, and genetic engineering of Anopheles mosquito vector.",
    "summary": "No abstract available via summary API. Please click link to read.",
    "link": "https://pubmed.ncbi.nlm.nih.gov/36842543/",
    "source": "PubMed",
    "topic": "CRISPR",
    "dateAdded": "2026-02-02T13:24:59.255Z"
  },
  {
    "id": "http://arxiv.org/abs/2601.01320v1",
    "title": "Adaptive Hierarchical Evaluation of LLMs and SAST tools for CWE Prediction in Python",
    "summary": "Large Language Models have become integral to software development, yet they frequently generate vulnerable code. Existing code vulnerability detection benchmarks employ binary classification, lacking the CWE-level specificity required for actionable feedback in iterative correction systems. We present ALPHA (Adaptive Learning via Penalty in Hierarchical Assessment), the first function-level Python benchmark that evaluates both LLMs and SAST tools using hierarchically aware, CWE-specific penalties. ALPHA distinguishes between over-generalisation, over-specification, and lateral errors, reflecting practical differences in diagnostic utility. Evaluating seven LLMs and two SAST tools, we find LLMs substantially outperform SAST, though SAST demonstrates higher precision when detections occur. Critically, prediction consistency varies dramatically across models (8.26%-81.87% agreement), with significant implications for feedback-driven systems. We further outline a pathway for future work incorporating ALPHA penalties into supervised fine-tuning, which could provide principled hierarchy-aware vulnerability detection pending empirical validation.",
    "link": "http://arxiv.org/abs/2601.01320v1",
    "source": "arXiv",
    "topic": "Python",
    "dateAdded": "2026-02-02T13:24:59.532Z"
  },
  {
    "id": "http://arxiv.org/abs/2503.16079v2",
    "title": "Efficient Data Ingestion in Cloud-based architecture: a Data Engineering Design Pattern Proposal",
    "summary": "In today's fast-paced digital world, data has become a critical asset for enterprises across various industries. However, the exponential growth of data presents significant challenges in managing and utilizing the vast amounts of information collected. Data engineering has emerged as a vital discipline addressing these challenges by providing robust platforms for effective data management, processing, and utilization. Data Engineering Patterns (DEP) refer to standardized practices and procedures in data engineering, such as ETL (extract, transform, load) processes, data pipelining, and data streaming management. Data Engineering Design Patterns (DEDP) are best practice solutions to common problems in data engineering, involving established, tested, and optimized approaches. These include architectural decisions, data modeling techniques, and data storage and retrieval strategies. While many researchers and practitioners have identified various DEPs and proposed DEDPs, such as data mesh and lambda architecture, the challenge of high-volume data ingestion remains inadequately addressed. In this paper, we propose a data ingestion design pattern for big data in cloud architecture, incorporating both incremental and full refresh techniques. Our approach leverages a flexible, metadata-driven framework to enhance feasibility and flexibility. This allows for easy changes to the ingestion type, schema modifications, table additions, and the integration of new data sources, all with minimal effort from data engineers. Tested on the Azure cloud architecture, our experiments demonstrate that the proposed techniques significantly reduce data ingestion time. Overall, this paper advances data management practices by presenting a detailed exploration of data ingestion challenges and defining a proposal for an effective design patterns for cloud-based architectures.",
    "link": "http://arxiv.org/abs/2503.16079v2",
    "source": "arXiv",
    "topic": "Data Engineering",
    "dateAdded": "2026-02-09T13:29:09.440Z"
  },
  {
    "id": "40598996",
    "title": "Development of a CRISPR/Cas9 RNP-mediated genetic engineering system in Paecilomyces variotii.",
    "summary": "No abstract available via summary API. Please click link to read.",
    "link": "https://pubmed.ncbi.nlm.nih.gov/40598996/",
    "source": "PubMed",
    "topic": "CRISPR",
    "dateAdded": "2026-02-09T13:29:09.665Z"
  },
  {
    "id": "http://arxiv.org/abs/2505.21575v1",
    "title": "StreamLink: Large-Language-Model Driven Distributed Data Engineering System",
    "summary": "Large Language Models (LLMs) have shown remarkable proficiency in natural language understanding (NLU), opening doors for innovative applications. We introduce StreamLink - an LLM-driven distributed data system designed to improve the efficiency and accessibility of data engineering tasks. We build StreamLink on top of distributed frameworks such as Apache Spark and Hadoop to handle large data at scale. One of the important design philosophies of StreamLink is to respect user data privacy by utilizing local fine-tuned LLMs instead of a public AI service like ChatGPT. With help from domain-adapted LLMs, we can improve our system's understanding of natural language queries from users in various scenarios and simplify the procedure of generating database queries like the Structured Query Language (SQL) for information processing. We also incorporate LLM-based syntax and security checkers to guarantee the reliability and safety of each generated query. StreamLink illustrates the potential of merging generative LLMs with distributed data processing for comprehensive and user-centric data engineering. With this architecture, we allow users to interact with complex database systems at different scales in a user-friendly and security-ensured manner, where the SQL generation reaches over 10\\% of execution accuracy compared to baseline methods, and allow users to find the most concerned item from hundreds of millions of items within a few seconds using natural language.",
    "link": "http://arxiv.org/abs/2505.21575v1",
    "source": "arXiv",
    "topic": "Data Engineering",
    "dateAdded": "2026-02-16T13:26:50.339Z"
  },
  {
    "id": "40770192",
    "title": "CRISPR.BOT an autonomous platform for streamlined genetic engineering and molecular biology applications.",
    "summary": "No abstract available via summary API. Please click link to read.",
    "link": "https://pubmed.ncbi.nlm.nih.gov/40770192/",
    "source": "PubMed",
    "topic": "CRISPR",
    "dateAdded": "2026-02-16T13:26:50.701Z"
  },
  {
    "id": "http://arxiv.org/abs/2601.01413v1",
    "title": "GlycoPy: An Equation-Oriented and Object-Oriented Software for Hierarchical Modeling, Optimization, and Control in Python",
    "summary": "Most existing model predictive control (MPC) applications in process industries employ lin-ear models, although real-world (bio)chemical processes are typically nonlinear. The use of linear models limits the performance and applicability of MPC for processes that span a wide range of operating conditions. A challenge in employing nonlinear models in MPC for com-plex systems is the lack of tools that facilitate hierarchical model development, as well as lack of efficient implementations of the corresponding nonlinear MPC (NMPC) algorithms. As a step towards making NMPC more practical for hierarchical systems, we introduce Gly-coPy, an equation-oriented, object-oriented software framework for process modeling, opti-mization, and NMPC in Python. GlycoPy enables users to focus on writing equations for modeling while supporting hierarchical modeling. GlycoPy includes algorithms for parame-ter estimation, dynamic optimization, and NMPC, and allows users to customize the simula-tion, optimization, and control algorithms. Three case studies, ranging from a simple differ-ential algebraic equation system to a multiscale bioprocess model, validate the modeling, optimization, and NMPC capabilities of GlycoPy. GlycoPy has the potential to bridge the gap between advanced NMPC algorithms and their practical application in real-world (bio)chemical processes.",
    "link": "http://arxiv.org/abs/2601.01413v1",
    "source": "arXiv",
    "topic": "Python",
    "dateAdded": "2026-02-16T13:26:50.940Z"
  }
]
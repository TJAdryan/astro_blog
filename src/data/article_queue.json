[
  {
    "id": "http://arxiv.org/abs/1710.04144v1",
    "title": "GUIDES - Geospatial Urban Infrastructure Data Engineering Solutions",
    "summary": "As the underground infrastructure systems of cities age, maintenance and repair become an increasing concern. Cities face difficulties in planning maintenance, predicting and responding to infrastructure related issues, and in realizing their vision to be a smart city due to their incomplete understanding of the existing state of the infrastructure. Only few cities have accurate and complete digital information on their underground infrastructure (e.g., electricity, water, natural gas) systems, which poses problems to those planning and performing construction projects. To address these issues, we introduce GUIDES as a new data conversion and management framework for urban underground infrastructure systems that enable city administrators, workers, and contractors along with the general public and other users to query digitized and integrated data to make smarter decisions. This demo paper presents the GUIDES architecture and describes two of its central components: (i) mapping of underground infrastructure systems, and (ii) integration of heterogeneous geospatial data.",
    "link": "http://arxiv.org/abs/1710.04144v1",
    "source": "arXiv",
    "topic": "Data Engineering",
    "dateAdded": "2026-01-04T21:42:09.291Z"
  },
  {
    "id": "http://arxiv.org/abs/1501.04832v1",
    "title": "Big Data: How Geo-information Helped Shape the Future of Data Engineering",
    "summary": "Very large data sets are the common rule in automated mapping, GIS, remote sensing, and what we can name geo-information. Indeed, in 1983 Landsat was already delivering gigabytes of data, and other sensors were in orbit or ready for launch, and a tantamount of cartographic data was being digitized. The retrospective paper revisits several issues that geo-information sciences had to face from the early stages on, including: structure ( to bring some structure to the data registered from a sampled signal, metadata); processing (huge amounts of data for big computers and fast algorithms); uncertainty (the kinds of errors, their quantification); consistency (when merging different sources of data is logically allowed, and meaningful); ontologies (clear and agreed shared definitions, if any kind of decision should be based upon them). All these issues are the background of Internet queries, and the underlying technology has been shaped during those years when geo-information engineering emerged.",
    "link": "http://arxiv.org/abs/1501.04832v1",
    "source": "arXiv",
    "topic": "Data Engineering",
    "dateAdded": "2026-01-04T22:23:53.718Z"
  },
  {
    "id": "31095680",
    "title": "Simplified pipelines for genetic engineering of mammalian embryos by CRISPR-Cas9 electroporationâ€ .",
    "summary": "No abstract available via summary API. Please click link to read.",
    "link": "https://pubmed.ncbi.nlm.nih.gov/31095680/",
    "source": "PubMed",
    "topic": "CRISPR",
    "dateAdded": "2026-01-04T22:43:42.644Z"
  },
  {
    "id": "http://arxiv.org/abs/2507.13892v2",
    "title": "Towards Next Generation Data Engineering Pipelines",
    "summary": "Data engineering pipelines are a widespread way to provide high-quality data for all kinds of data science applications. However, numerous challenges still remain in the composition and operation of such pipelines. Data engineering pipelines do not always deliver high-quality data. By default, they are also not reactive to changes. When new data is coming in which deviates from prior data, the pipeline could crash or output undesired results. We therefore envision three levels of next generation data engineering pipelines: optimized data pipelines, self-aware data pipelines, and self-adapting data pipelines. Pipeline optimization addresses the composition of operators and their parametrization in order to achieve the highest possible data quality. Self-aware data engineering pipelines enable a continuous monitoring of its current state, notifying data engineers on significant changes. Self-adapting data engineering pipelines are then even able to automatically react to those changes. We propose approaches to achieve each of these levels.",
    "link": "http://arxiv.org/abs/2507.13892v2",
    "source": "arXiv",
    "topic": "Data Engineering",
    "dateAdded": "2026-01-19T13:17:03.323Z"
  },
  {
    "id": "37849926",
    "title": "Application of the endogenous CRISPR-Cas type I-D system for genetic engineering in the thermoacidophilic archaeon Sulfolobus acidocaldarius.",
    "summary": "No abstract available via summary API. Please click link to read.",
    "link": "https://pubmed.ncbi.nlm.nih.gov/37849926/",
    "source": "PubMed",
    "topic": "CRISPR",
    "dateAdded": "2026-01-19T13:17:04.230Z"
  },
  {
    "id": "http://arxiv.org/abs/2511.05821v1",
    "title": "PyGress: Tool for Analyzing the Progression of Code Proficiency in Python OSS Projects",
    "summary": "Assessing developer proficiency in open-source software (OSS) projects is essential for understanding project dynamics, especially for expertise. This paper presents PyGress, a web-based tool designed to automatically evaluate and visualize Python code proficiency using pycefr, a Python code proficiency analyzer. By submitting a GitHub repository link, the system extracts commit histories, analyzes source code proficiency across CEFR-aligned levels (A1 to C2), and generates visual summaries of individual and project-wide proficiency. The PyGress tool visualizes per-contributor proficiency distribution and tracks project code proficiency progression over time. PyGress offers an interactive way to explore contributor coding levels in Python OSS repositories. The video demonstration of the PyGress tool can be found at https://youtu.be/hxoeK-ggcWk, and the source code of the tool is publicly available at https://github.com/MUICT-SERU/PyGress.",
    "link": "http://arxiv.org/abs/2511.05821v1",
    "source": "arXiv",
    "topic": "Python",
    "dateAdded": "2026-01-19T13:17:04.456Z"
  }
]